{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "############################################################\n",
    "#  MaskRCNN Class\n",
    "############################################################\n",
    "\n",
    "class MaskRCNN():\n",
    "    \"\"\"Encapsulates the Mask RCNN model functionality.\n",
    "\n",
    "    The actual Keras model is in the keras_model property.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode, config, model_dir):\n",
    "        \"\"\"\n",
    "        mode: Either \"training\" or \"inference\"\n",
    "        config: A Sub-class of the Config class\n",
    "        model_dir: Directory to save training logs and trained weights\n",
    "        \"\"\"\n",
    "        assert mode in ['training', 'inference']\n",
    "        self.mode = mode\n",
    "        self.config = config\n",
    "        self.model_dir = model_dir\n",
    "        self.set_log_dir()\n",
    "        self.keras_model = self.build(mode=mode, config=config)\n",
    "\n",
    "    def build(self, mode, config):\n",
    "        \"\"\"Build Mask R-CNN architecture.\n",
    "            input_shape: The shape of the input image.\n",
    "            mode: Either \"training\" or \"inference\". The inputs and\n",
    "                outputs of the model differ accordingly.\n",
    "        \"\"\"\n",
    "        assert mode in ['training', 'inference']\n",
    "\n",
    "        # Image size must be dividable by 2 multiple times\n",
    "        h, w = config.IMAGE_SHAPE[:2]\n",
    "        if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
    "            raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                            \"to avoid fractions when downscaling and upscaling.\"\n",
    "                            \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "\n",
    "        # Inputs\n",
    "        input_image = KL.Input(\n",
    "            shape=[None, None, config.IMAGE_SHAPE[2]], name=\"input_image\")\n",
    "        input_image_meta = KL.Input(shape=[config.IMAGE_META_SIZE],\n",
    "                                    name=\"input_image_meta\")\n",
    "        if mode == \"training\":\n",
    "            # RPN GT\n",
    "            input_rpn_match = KL.Input(\n",
    "                shape=[None, 1], name=\"input_rpn_match\", dtype=tf.int32)\n",
    "            input_rpn_bbox = KL.Input(\n",
    "                shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\n",
    "\n",
    "            # Detection GT (class IDs, bounding boxes, and masks)\n",
    "            # 1. GT Class IDs (zero padded)\n",
    "            input_gt_class_ids = KL.Input(\n",
    "                shape=[None], name=\"input_gt_class_ids\", dtype=tf.int32)\n",
    "            # 2. GT Boxes in pixels (zero padded)\n",
    "            # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates\n",
    "            input_gt_boxes = KL.Input(\n",
    "                shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\n",
    "            # Normalize coordinates\n",
    "            gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(\n",
    "                x, K.shape(input_image)[1:3]))(input_gt_boxes)\n",
    "            # 3. GT Masks (zero padded)\n",
    "            # [batch, height, width, MAX_GT_INSTANCES]\n",
    "            if config.USE_MINI_MASK:\n",
    "                input_gt_masks = KL.Input(\n",
    "                    shape=[config.MINI_MASK_SHAPE[0],\n",
    "                           config.MINI_MASK_SHAPE[1], None],\n",
    "                    name=\"input_gt_masks\", dtype=bool)\n",
    "            else:\n",
    "                input_gt_masks = KL.Input(\n",
    "                    shape=[config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1], None],\n",
    "                    name=\"input_gt_masks\", dtype=bool)\n",
    "        elif mode == \"inference\":\n",
    "            # Anchors in normalized coordinates\n",
    "            input_anchors = KL.Input(shape=[None, 4], name=\"input_anchors\")\n",
    "\n",
    "        # Build the shared convolutional layers.\n",
    "        # Bottom-up Layers\n",
    "        # Returns a list of the last layers of each stage, 5 in total.\n",
    "        # Don't create the thead (stage 5), so we pick the 4th item in the list.\n",
    "        if callable(config.BACKBONE):\n",
    "            _, C2, C3, C4, C5 = config.BACKBONE(input_image, stage5=True,\n",
    "                                                train_bn=config.TRAIN_BN)\n",
    "        else:\n",
    "            _, C2, C3, C4, C5 = resnet_graph(input_image, config.BACKBONE,\n",
    "                                             stage5=True, train_bn=config.TRAIN_BN)\n",
    "        # Top-down Layers\n",
    "        # TODO: add assert to varify feature map sizes match what's in config\n",
    "        P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')(C5)\n",
    "        P4 = KL.Add(name=\"fpn_p4add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\n",
    "            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')(C4)])\n",
    "        P3 = KL.Add(name=\"fpn_p3add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\n",
    "            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')(C3)])\n",
    "        P2 = KL.Add(name=\"fpn_p2add\")([\n",
    "            KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\n",
    "            KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')(C2)])\n",
    "        # Attach 3x3 conv to all P layers to get the final feature maps.\n",
    "        P2 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n",
    "        P3 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n",
    "        P4 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n",
    "        P5 = KL.Conv2D(config.TOP_DOWN_PYRAMID_SIZE, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\n",
    "        # P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "        # subsampling from P5 with stride of 2.\n",
    "        P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\n",
    "\n",
    "        # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "        rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
    "        mrcnn_feature_maps = [P2, P3, P4, P5]\n",
    "\n",
    "        # Anchors\n",
    "        if mode == \"training\":\n",
    "            anchors = self.get_anchors(config.IMAGE_SHAPE)\n",
    "            # Duplicate across the batch dimension because Keras requires it\n",
    "            # TODO: can this be optimized to avoid duplicating the anchors?\n",
    "            anchors = np.broadcast_to(anchors, (config.BATCH_SIZE,) + anchors.shape)\n",
    "            # A hack to get around Keras's bad support for constants\n",
    "            anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=\"anchors\")(input_image)\n",
    "        else:\n",
    "            anchors = input_anchors\n",
    "\n",
    "        # RPN Model\n",
    "        rpn = build_rpn_model(config.RPN_ANCHOR_STRIDE,\n",
    "                              len(config.RPN_ANCHOR_RATIOS), config.TOP_DOWN_PYRAMID_SIZE)\n",
    "        # Loop through pyramid layers\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in rpn_feature_maps:\n",
    "            layer_outputs.append(rpn([p]))\n",
    "        # Concatenate layer outputs\n",
    "        # Convert from list of lists of level outputs to list of lists\n",
    "        # of outputs across levels.\n",
    "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "        output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [KL.Concatenate(axis=1, name=n)(list(o))\n",
    "                   for o, n in zip(outputs, output_names)]\n",
    "\n",
    "        rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "\n",
    "        # Generate proposals\n",
    "        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "        # and zero padded.\n",
    "        proposal_count = config.POST_NMS_ROIS_TRAINING if mode == \"training\"\\\n",
    "            else config.POST_NMS_ROIS_INFERENCE\n",
    "        rpn_rois = ProposalLayer(\n",
    "            proposal_count=proposal_count,\n",
    "            nms_threshold=config.RPN_NMS_THRESHOLD,\n",
    "            name=\"ROI\",\n",
    "            config=config)([rpn_class, rpn_bbox, anchors])\n",
    "\n",
    "        if mode == \"training\":\n",
    "            # Class ID mask to mark class IDs supported by the dataset the image\n",
    "            # came from.\n",
    "            active_class_ids = KL.Lambda(\n",
    "                lambda x: parse_image_meta_graph(x)[\"active_class_ids\"]\n",
    "                )(input_image_meta)\n",
    "\n",
    "            if not config.USE_RPN_ROIS:\n",
    "                # Ignore predicted ROIs and use ROIs provided as an input.\n",
    "                input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n",
    "                                      name=\"input_roi\", dtype=np.int32)\n",
    "                # Normalize coordinates\n",
    "                target_rois = KL.Lambda(lambda x: norm_boxes_graph(\n",
    "                    x, K.shape(input_image)[1:3]))(input_rois)\n",
    "            else:\n",
    "                target_rois = rpn_rois\n",
    "\n",
    "            # Generate detection targets\n",
    "            # Subsamples proposals and generates target outputs for training\n",
    "            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
    "            # padded. Equally, returned rois and targets are zero padded.\n",
    "            rois, target_class_ids, target_bbox, target_mask =\\\n",
    "                DetectionTargetLayer(config, name=\"proposal_targets\")([\n",
    "                    target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])\n",
    "\n",
    "            # Network Heads\n",
    "            # TODO: verify that this handles zero padded ROIs\n",
    "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n",
    "                fpn_classifier_graph(rois, mrcnn_feature_maps, input_image_meta,\n",
    "                                     config.POOL_SIZE, config.NUM_CLASSES,\n",
    "                                     train_bn=config.TRAIN_BN,\n",
    "                                     fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)\n",
    "\n",
    "            mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps,\n",
    "                                              input_image_meta,\n",
    "                                              config.MASK_POOL_SIZE,\n",
    "                                              config.NUM_CLASSES,\n",
    "                                              train_bn=config.TRAIN_BN)\n",
    "\n",
    "            # TODO: clean up (use tf.identify if necessary)\n",
    "            output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois)\n",
    "\n",
    "            # Losses\n",
    "            rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\n",
    "                [input_rpn_match, rpn_class_logits])\n",
    "            rpn_bbox_loss = KL.Lambda(lambda x: rpn_bbox_loss_graph(config, *x), name=\"rpn_bbox_loss\")(\n",
    "                [input_rpn_bbox, input_rpn_match, rpn_bbox])\n",
    "            class_loss = KL.Lambda(lambda x: mrcnn_class_loss_graph(*x), name=\"mrcnn_class_loss\")(\n",
    "                [target_class_ids, mrcnn_class_logits, active_class_ids])\n",
    "            bbox_loss = KL.Lambda(lambda x: mrcnn_bbox_loss_graph(*x), name=\"mrcnn_bbox_loss\")(\n",
    "                [target_bbox, target_class_ids, mrcnn_bbox])\n",
    "            mask_loss = KL.Lambda(lambda x: mrcnn_mask_loss_graph(*x), name=\"mrcnn_mask_loss\")(\n",
    "                [target_mask, target_class_ids, mrcnn_mask])\n",
    "\n",
    "            # Model\n",
    "            inputs = [input_image, input_image_meta,\n",
    "                      input_rpn_match, input_rpn_bbox, input_gt_class_ids, input_gt_boxes, input_gt_masks]\n",
    "            if not config.USE_RPN_ROIS:\n",
    "                inputs.append(input_rois)\n",
    "            outputs = [rpn_class_logits, rpn_class, rpn_bbox,\n",
    "                       mrcnn_class_logits, mrcnn_class, mrcnn_bbox, mrcnn_mask,\n",
    "                       rpn_rois, output_rois,\n",
    "                       rpn_class_loss, rpn_bbox_loss, class_loss, bbox_loss, mask_loss]\n",
    "            model = KM.Model(inputs, outputs, name='mask_rcnn')\n",
    "        else:\n",
    "            # Network Heads\n",
    "            # Proposal classifier and BBox regressor heads\n",
    "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox =\\\n",
    "                fpn_classifier_graph(rpn_rois, mrcnn_feature_maps, input_image_meta,\n",
    "                                     config.POOL_SIZE, config.NUM_CLASSES,\n",
    "                                     train_bn=config.TRAIN_BN,\n",
    "                                     fc_layers_size=config.FPN_CLASSIF_FC_LAYERS_SIZE)\n",
    "\n",
    "            # Detections\n",
    "            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in\n",
    "            # normalized coordinates\n",
    "            detections = DetectionLayer(config, name=\"mrcnn_detection\")(\n",
    "                [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\n",
    "\n",
    "            # Create masks for detections\n",
    "            detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections)\n",
    "            mrcnn_mask = build_fpn_mask_graph(detection_boxes, mrcnn_feature_maps,\n",
    "                                              input_image_meta,\n",
    "                                              config.MASK_POOL_SIZE,\n",
    "                                              config.NUM_CLASSES,\n",
    "                                              train_bn=config.TRAIN_BN)\n",
    "\n",
    "            model = KM.Model([input_image, input_image_meta, input_anchors],\n",
    "                             [detections, mrcnn_class, mrcnn_bbox,\n",
    "                                 mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],\n",
    "                             name='mask_rcnn')\n",
    "\n",
    "        # Add multi-GPU support.\n",
    "        if config.GPU_COUNT > 1:\n",
    "            from mrcnn.parallel_model import ParallelModel\n",
    "            model = ParallelModel(model, config.GPU_COUNT)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def find_last(self):\n",
    "        \"\"\"Finds the last checkpoint file of the last trained model in the\n",
    "        model directory.\n",
    "        Returns:\n",
    "            The path of the last checkpoint file\n",
    "        \"\"\"\n",
    "        # Get directory names. Each directory corresponds to a model\n",
    "        dir_names = next(os.walk(self.model_dir))[1]\n",
    "        key = self.config.NAME.lower()\n",
    "        dir_names = filter(lambda f: f.startswith(key), dir_names)\n",
    "        dir_names = sorted(dir_names)\n",
    "        if not dir_names:\n",
    "            import errno\n",
    "            raise FileNotFoundError(\n",
    "                errno.ENOENT,\n",
    "                \"Could not find model directory under {}\".format(self.model_dir))\n",
    "        # Pick last directory\n",
    "        dir_name = os.path.join(self.model_dir, dir_names[-1])\n",
    "        # Find the last checkpoint\n",
    "        checkpoints = next(os.walk(dir_name))[2]\n",
    "        checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n",
    "        checkpoints = sorted(checkpoints)\n",
    "        if not checkpoints:\n",
    "            import errno\n",
    "            raise FileNotFoundError(\n",
    "                errno.ENOENT, \"Could not find weight files in {}\".format(dir_name))\n",
    "        checkpoint = os.path.join(dir_name, checkpoints[-1])\n",
    "        return checkpoint\n",
    "\n",
    "    def load_weights(self, filepath, by_name=False, exclude=None):\n",
    "        \"\"\"Modified version of the corresponding Keras function with\n",
    "        the addition of multi-GPU support and the ability to exclude\n",
    "        some layers from loading.\n",
    "        exclude: list of layer names to exclude\n",
    "        \"\"\"\n",
    "        import h5py\n",
    "        # Conditional import to support versions of Keras before 2.2\n",
    "        # TODO: remove in about 6 months (end of 2018)\n",
    "        try:\n",
    "            from keras.engine import saving\n",
    "        except ImportError:\n",
    "            # Keras before 2.2 used the 'topology' namespace.\n",
    "            from keras.engine import topology as saving\n",
    "\n",
    "        if exclude:\n",
    "            by_name = True\n",
    "\n",
    "        if h5py is None:\n",
    "            raise ImportError('`load_weights` requires h5py.')\n",
    "        f = h5py.File(filepath, mode='r')\n",
    "        if 'layer_names' not in f.attrs and 'model_weights' in f:\n",
    "            f = f['model_weights']\n",
    "\n",
    "        # In multi-GPU training, we wrap the model. Get layers\n",
    "        # of the inner model because they have the weights.\n",
    "        keras_model = self.keras_model\n",
    "        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\n",
    "            else keras_model.layers\n",
    "\n",
    "        # Exclude some layers\n",
    "        if exclude:\n",
    "            layers = filter(lambda l: l.name not in exclude, layers)\n",
    "\n",
    "        if by_name:\n",
    "            saving.load_weights_from_hdf5_group_by_name(f, layers)\n",
    "        else:\n",
    "            saving.load_weights_from_hdf5_group(f, layers)\n",
    "        if hasattr(f, 'close'):\n",
    "            f.close()\n",
    "\n",
    "        # Update the log directory\n",
    "        self.set_log_dir(filepath)\n",
    "\n",
    "    def get_imagenet_weights(self):\n",
    "        \"\"\"Downloads ImageNet trained weights from Keras.\n",
    "        Returns path to weights file.\n",
    "        \"\"\"\n",
    "        from keras.utils.data_utils import get_file\n",
    "        TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/'\\\n",
    "                                 'releases/download/v0.2/'\\\n",
    "                                 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                TF_WEIGHTS_PATH_NO_TOP,\n",
    "                                cache_subdir='models',\n",
    "                                md5_hash='a268eb855778b3df3c7506639542a6af')\n",
    "        return weights_path\n",
    "\n",
    "    def compile(self, learning_rate, momentum):\n",
    "        \"\"\"Gets the model ready for training. Adds losses, regularization, and\n",
    "        metrics. Then calls the Keras compile() function.\n",
    "        \"\"\"\n",
    "        # Optimizer object\n",
    "        optimizer = keras.optimizers.SGD(\n",
    "            lr=learning_rate, momentum=momentum,\n",
    "            clipnorm=self.config.GRADIENT_CLIP_NORM)\n",
    "        # Add Losses\n",
    "        # First, clear previously set losses to avoid duplication\n",
    "        self.keras_model._losses = []\n",
    "        self.keras_model._per_input_losses = {}\n",
    "        loss_names = [\n",
    "            \"rpn_class_loss\",  \"rpn_bbox_loss\",\n",
    "            \"mrcnn_class_loss\", \"mrcnn_bbox_loss\", \"mrcnn_mask_loss\"]\n",
    "        for name in loss_names:\n",
    "            layer = self.keras_model.get_layer(name)\n",
    "            if layer.output in self.keras_model.losses:\n",
    "                continue\n",
    "            loss = (\n",
    "                tf.reduce_mean(layer.output, keepdims=True)\n",
    "                * self.config.LOSS_WEIGHTS.get(name, 1.))\n",
    "            self.keras_model.add_loss(loss)\n",
    "\n",
    "        # Add L2 Regularization\n",
    "        # Skip gamma and beta weights of batch normalization layers.\n",
    "        reg_losses = [\n",
    "            keras.regularizers.l2(self.config.WEIGHT_DECAY)(w) / tf.cast(tf.size(w), tf.float32)\n",
    "            for w in self.keras_model.trainable_weights\n",
    "            if 'gamma' not in w.name and 'beta' not in w.name]\n",
    "        self.keras_model.add_loss(tf.add_n(reg_losses))\n",
    "\n",
    "        # Compile\n",
    "        self.keras_model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=[None] * len(self.keras_model.outputs))\n",
    "\n",
    "        # Add metrics for losses\n",
    "        for name in loss_names:\n",
    "            if name in self.keras_model.metrics_names:\n",
    "                continue\n",
    "            layer = self.keras_model.get_layer(name)\n",
    "            self.keras_model.metrics_names.append(name)\n",
    "            loss = (\n",
    "                tf.reduce_mean(layer.output, keepdims=True)\n",
    "                * self.config.LOSS_WEIGHTS.get(name, 1.))\n",
    "            #self.keras_model.metrics_tensors.append(loss)\n",
    "            self.keras_model.add_metric(loss, name)\n",
    "\n",
    "    def set_trainable(self, layer_regex, keras_model=None, indent=0, verbose=1):\n",
    "        \"\"\"Sets model layers as trainable if their names match\n",
    "        the given regular expression.\n",
    "        \"\"\"\n",
    "        # Print message on the first call (but not on recursive calls)\n",
    "        if verbose > 0 and keras_model is None:\n",
    "            log(\"Selecting layers to train\")\n",
    "\n",
    "        keras_model = keras_model or self.keras_model\n",
    "\n",
    "        # In multi-GPU training, we wrap the model. Get layers\n",
    "        # of the inner model because they have the weights.\n",
    "        layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\")\\\n",
    "            else keras_model.layers\n",
    "\n",
    "        for layer in layers:\n",
    "            # Is the layer a model?\n",
    "            if layer.__class__.__name__ == 'Model':\n",
    "                print(\"In model: \", layer.name)\n",
    "                self.set_trainable(\n",
    "                    layer_regex, keras_model=layer, indent=indent + 4)\n",
    "                continue\n",
    "\n",
    "            if not layer.weights:\n",
    "                continue\n",
    "            # Is it trainable?\n",
    "            trainable = bool(re.fullmatch(layer_regex, layer.name))\n",
    "            # Update layer. If layer is a container, update inner layer.\n",
    "            if layer.__class__.__name__ == 'TimeDistributed':\n",
    "                layer.layer.trainable = trainable\n",
    "            else:\n",
    "                layer.trainable = trainable\n",
    "            # Print trainable layer names\n",
    "            if trainable and verbose > 0:\n",
    "                log(\"{}{:20}   ({})\".format(\" \" * indent, layer.name,\n",
    "                                            layer.__class__.__name__))\n",
    "\n",
    "    def set_log_dir(self, model_path=None):\n",
    "        \"\"\"Sets the model log directory and epoch counter.\n",
    "\n",
    "        model_path: If None, or a format different from what this code uses\n",
    "            then set a new log directory and start epochs from 0. Otherwise,\n",
    "            extract the log directory and the epoch counter from the file\n",
    "            name.\n",
    "        \"\"\"\n",
    "        # Set date and epoch counter as if starting a new model\n",
    "        self.epoch = 0\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        # If we have a model path with date and epochs use them\n",
    "        if model_path:\n",
    "            # Continue from we left of. Get epoch and date from the file name\n",
    "            # A sample model path might look like:\n",
    "            # \\path\\to\\logs\\coco20171029T2315\\mask_rcnn_coco_0001.h5 (Windows)\n",
    "            # /path/to/logs/coco20171029T2315/mask_rcnn_coco_0001.h5 (Linux)\n",
    "            regex = r\".*[/\\\\][\\w-]+(\\d{4})(\\d{2})(\\d{2})T(\\d{2})(\\d{2})[/\\\\]mask\\_rcnn\\_[\\w-]+(\\d{4})\\.h5\"\n",
    "            m = re.match(regex, model_path)\n",
    "            if m:\n",
    "                now = datetime.datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)),\n",
    "                                        int(m.group(4)), int(m.group(5)))\n",
    "                # Epoch number in file is 1-based, and in Keras code it's 0-based.\n",
    "                # So, adjust for that then increment by one to start from the next epoch\n",
    "                self.epoch = int(m.group(6)) - 1 + 1\n",
    "                print('Re-starting from epoch %d' % self.epoch)\n",
    "\n",
    "        # Directory for training logs\n",
    "        self.log_dir = os.path.join(self.model_dir, \"{}{:%Y%m%dT%H%M}\".format(\n",
    "            self.config.NAME.lower(), now))\n",
    "\n",
    "        # Path to save after each epoch. Include placeholders that get filled by Keras.\n",
    "        self.checkpoint_path = os.path.join(self.log_dir, \"mask_rcnn_{}_*epoch*.h5\".format(\n",
    "            self.config.NAME.lower()))\n",
    "        self.checkpoint_path = self.checkpoint_path.replace(\n",
    "            \"*epoch*\", \"{epoch:04d}\")\n",
    "\n",
    "    def train(self, train_dataset, val_dataset, learning_rate, epochs, layers,\n",
    "              augmentation=None, custom_callbacks=None, no_augmentation_sources=None):\n",
    "        \"\"\"Train the model.\n",
    "        train_dataset, val_dataset: Training and validation Dataset objects.\n",
    "        learning_rate: The learning rate to train with\n",
    "        epochs: Number of training epochs. Note that previous training epochs\n",
    "                are considered to be done alreay, so this actually determines\n",
    "                the epochs to train in total rather than in this particaular\n",
    "                call.\n",
    "        layers: Allows selecting wich layers to train. It can be:\n",
    "            - A regular expression to match layer names to train\n",
    "            - One of these predefined values:\n",
    "              heads: The RPN, classifier and mask heads of the network\n",
    "              all: All the layers\n",
    "              3+: Train Resnet stage 3 and up\n",
    "              4+: Train Resnet stage 4 and up\n",
    "              5+: Train Resnet stage 5 and up\n",
    "        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug)\n",
    "            augmentation. For example, passing imgaug.augmenters.Fliplr(0.5)\n",
    "            flips images right/left 50% of the time. You can pass complex\n",
    "            augmentations as well. This augmentation applies 50% of the\n",
    "            time, and when it does it flips images right/left half the time\n",
    "            and adds a Gaussian blur with a random sigma in range 0 to 5.\n",
    "\n",
    "                augmentation = imgaug.augmenters.Sometimes(0.5, [\n",
    "                    imgaug.augmenters.Fliplr(0.5),\n",
    "                    imgaug.augmenters.GaussianBlur(sigma=(0.0, 5.0))\n",
    "                ])\n",
    "\t    custom_callbacks: Optional. Add custom callbacks to be called\n",
    "\t        with the keras fit_generator method. Must be list of type keras.callbacks.\n",
    "        no_augmentation_sources: Optional. List of sources to exclude for\n",
    "            augmentation. A source is string that identifies a dataset and is\n",
    "            defined in the Dataset class.\n",
    "        \"\"\"\n",
    "        assert self.mode == \"training\", \"Create model in training mode.\"\n",
    "\n",
    "        # Pre-defined layer regular expressions\n",
    "        layer_regex = {\n",
    "            # all layers but the backbone\n",
    "            \"heads\": r\"(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            # From a specific Resnet stage and up\n",
    "            \"3+\": r\"(res3.*)|(bn3.*)|(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            \"4+\": r\"(res4.*)|(bn4.*)|(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            \"5+\": r\"(res5.*)|(bn5.*)|(mrcnn\\_.*)|(rpn\\_.*)|(fpn\\_.*)\",\n",
    "            # All layers\n",
    "            \"all\": \".*\",\n",
    "        }\n",
    "        if layers in layer_regex.keys():\n",
    "            layers = layer_regex[layers]\n",
    "\n",
    "        # Data generators\n",
    "        train_generator = data_generator(train_dataset, self.config, shuffle=True,\n",
    "                                         augmentation=augmentation,\n",
    "                                         batch_size=self.config.BATCH_SIZE,\n",
    "                                         no_augmentation_sources=no_augmentation_sources)\n",
    "        val_generator = data_generator(val_dataset, self.config, shuffle=True,\n",
    "                                       batch_size=self.config.BATCH_SIZE)\n",
    "\n",
    "        # Create log_dir if it does not exist\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.TensorBoard(log_dir=self.log_dir,\n",
    "                                        histogram_freq=0, write_graph=True, write_images=False),\n",
    "            keras.callbacks.ModelCheckpoint(self.checkpoint_path,\n",
    "                                            verbose=0, save_weights_only=True),\n",
    "        ]\n",
    "\n",
    "        # Add custom callbacks to the list\n",
    "        if custom_callbacks:\n",
    "            callbacks += custom_callbacks\n",
    "\n",
    "        # Train\n",
    "        log(\"\\nStarting at epoch {}. LR={}\\n\".format(self.epoch, learning_rate))\n",
    "        log(\"Checkpoint Path: {}\".format(self.checkpoint_path))\n",
    "        self.set_trainable(layers)\n",
    "        self.compile(learning_rate, self.config.LEARNING_MOMENTUM)\n",
    "\n",
    "        # Work-around for Windows: Keras fails on Windows when using\n",
    "        # multiprocessing workers. See discussion here:\n",
    "        # https://github.com/matterport/Mask_RCNN/issues/13#issuecomment-353124009\n",
    "        if os.name is 'nt':\n",
    "            workers = 0\n",
    "        else:\n",
    "            workers = multiprocessing.cpu_count()\n",
    "\n",
    "        self.keras_model.fit_generator(\n",
    "            train_generator,\n",
    "            initial_epoch=self.epoch,\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=self.config.STEPS_PER_EPOCH,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=self.config.VALIDATION_STEPS,\n",
    "            max_queue_size=100,\n",
    "            workers=workers,\n",
    "            use_multiprocessing=True,\n",
    "        )\n",
    "        self.epoch = max(self.epoch, epochs)\n",
    "\n",
    "    def mold_inputs(self, images):\n",
    "        \"\"\"Takes a list of images and modifies them to the format expected\n",
    "        as an input to the neural network.\n",
    "        images: List of image matrices [height,width,depth]. Images can have\n",
    "            different sizes.\n",
    "\n",
    "        Returns 3 Numpy matrices:\n",
    "        molded_images: [N, h, w, 3]. Images resized and normalized.\n",
    "        image_metas: [N, length of meta data]. Details about each image.\n",
    "        windows: [N, (y1, x1, y2, x2)]. The portion of the image that has the\n",
    "            original image (padding excluded).\n",
    "        \"\"\"\n",
    "        molded_images = []\n",
    "        image_metas = []\n",
    "        windows = []\n",
    "        for image in images:\n",
    "            # Resize image\n",
    "            # TODO: move resizing to mold_image()\n",
    "            molded_image, window, scale, padding, crop = utils.resize_image(\n",
    "                image,\n",
    "                min_dim=self.config.IMAGE_MIN_DIM,\n",
    "                min_scale=self.config.IMAGE_MIN_SCALE,\n",
    "                max_dim=self.config.IMAGE_MAX_DIM,\n",
    "                mode=self.config.IMAGE_RESIZE_MODE)\n",
    "            molded_image = mold_image(molded_image, self.config)\n",
    "            # Build image_meta\n",
    "            image_meta = compose_image_meta(\n",
    "                0, image.shape, molded_image.shape, window, scale,\n",
    "                np.zeros([self.config.NUM_CLASSES], dtype=np.int32))\n",
    "            # Append\n",
    "            molded_images.append(molded_image)\n",
    "            windows.append(window)\n",
    "            image_metas.append(image_meta)\n",
    "        # Pack into arrays\n",
    "        molded_images = np.stack(molded_images)\n",
    "        image_metas = np.stack(image_metas)\n",
    "        windows = np.stack(windows)\n",
    "        return molded_images, image_metas, windows\n",
    "\n",
    "    def unmold_detections(self, detections, mrcnn_mask, original_image_shape,\n",
    "                          image_shape, window):\n",
    "        \"\"\"Reformats the detections of one image from the format of the neural\n",
    "        network output to a format suitable for use in the rest of the\n",
    "        application.\n",
    "\n",
    "        detections: [N, (y1, x1, y2, x2, class_id, score)] in normalized coordinates\n",
    "        mrcnn_mask: [N, height, width, num_classes]\n",
    "        original_image_shape: [H, W, C] Original image shape before resizing\n",
    "        image_shape: [H, W, C] Shape of the image after resizing and padding\n",
    "        window: [y1, x1, y2, x2] Pixel coordinates of box in the image where the real\n",
    "                image is excluding the padding.\n",
    "\n",
    "        Returns:\n",
    "        boxes: [N, (y1, x1, y2, x2)] Bounding boxes in pixels\n",
    "        class_ids: [N] Integer class IDs for each bounding box\n",
    "        scores: [N] Float probability scores of the class_id\n",
    "        masks: [height, width, num_instances] Instance masks\n",
    "        \"\"\"\n",
    "        # How many detections do we have?\n",
    "        # Detections array is padded with zeros. Find the first class_id == 0.\n",
    "        zero_ix = np.where(detections[:, 4] == 0)[0]\n",
    "        N = zero_ix[0] if zero_ix.shape[0] > 0 else detections.shape[0]\n",
    "\n",
    "        # Extract boxes, class_ids, scores, and class-specific masks\n",
    "        boxes = detections[:N, :4]\n",
    "        class_ids = detections[:N, 4].astype(np.int32)\n",
    "        scores = detections[:N, 5]\n",
    "        masks = mrcnn_mask[np.arange(N), :, :, class_ids]\n",
    "\n",
    "        # Translate normalized coordinates in the resized image to pixel\n",
    "        # coordinates in the original image before resizing\n",
    "        window = utils.norm_boxes(window, image_shape[:2])\n",
    "        wy1, wx1, wy2, wx2 = window\n",
    "        shift = np.array([wy1, wx1, wy1, wx1])\n",
    "        wh = wy2 - wy1  # window height\n",
    "        ww = wx2 - wx1  # window width\n",
    "        scale = np.array([wh, ww, wh, ww])\n",
    "        # Convert boxes to normalized coordinates on the window\n",
    "        boxes = np.divide(boxes - shift, scale)\n",
    "        # Convert boxes to pixel coordinates on the original image\n",
    "        boxes = utils.denorm_boxes(boxes, original_image_shape[:2])\n",
    "\n",
    "        # Filter out detections with zero area. Happens in early training when\n",
    "        # network weights are still random\n",
    "        exclude_ix = np.where(\n",
    "            (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]) <= 0)[0]\n",
    "        if exclude_ix.shape[0] > 0:\n",
    "            boxes = np.delete(boxes, exclude_ix, axis=0)\n",
    "            class_ids = np.delete(class_ids, exclude_ix, axis=0)\n",
    "            scores = np.delete(scores, exclude_ix, axis=0)\n",
    "            masks = np.delete(masks, exclude_ix, axis=0)\n",
    "            N = class_ids.shape[0]\n",
    "\n",
    "        # Resize masks to original image size and set boundary threshold.\n",
    "        full_masks = []\n",
    "        for i in range(N):\n",
    "            # Convert neural network mask to full size mask\n",
    "            full_mask = utils.unmold_mask(masks[i], boxes[i], original_image_shape)\n",
    "            full_masks.append(full_mask)\n",
    "        full_masks = np.stack(full_masks, axis=-1)\\\n",
    "            if full_masks else np.empty(original_image_shape[:2] + (0,))\n",
    "\n",
    "        return boxes, class_ids, scores, full_masks\n",
    "\n",
    "    def detect(self, images, verbose=0):\n",
    "        \"\"\"Runs the detection pipeline.\n",
    "\n",
    "        images: List of images, potentially of different sizes.\n",
    "\n",
    "        Returns a list of dicts, one dict per image. The dict contains:\n",
    "        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n",
    "        class_ids: [N] int class IDs\n",
    "        scores: [N] float probability scores for the class IDs\n",
    "        masks: [H, W, N] instance binary masks\n",
    "        \"\"\"\n",
    "        assert self.mode == \"inference\", \"Create model in inference mode.\"\n",
    "        assert len(\n",
    "            images) == self.config.BATCH_SIZE, \"len(images) must be equal to BATCH_SIZE\"\n",
    "\n",
    "        if verbose:\n",
    "            log(\"Processing {} images\".format(len(images)))\n",
    "            for image in images:\n",
    "                log(\"image\", image)\n",
    "\n",
    "        # Mold inputs to format expected by the neural network\n",
    "        molded_images, image_metas, windows = self.mold_inputs(images)\n",
    "\n",
    "        # Validate image sizes\n",
    "        # All images in a batch MUST be of the same size\n",
    "        image_shape = molded_images[0].shape\n",
    "        for g in molded_images[1:]:\n",
    "            assert g.shape == image_shape,\\\n",
    "                \"After resizing, all images must have the same size. Check IMAGE_RESIZE_MODE and image sizes.\"\n",
    "\n",
    "        # Anchors\n",
    "        anchors = self.get_anchors(image_shape)\n",
    "        # Duplicate across the batch dimension because Keras requires it\n",
    "        # TODO: can this be optimized to avoid duplicating the anchors?\n",
    "        anchors = np.broadcast_to(anchors, (self.config.BATCH_SIZE,) + anchors.shape)\n",
    "\n",
    "        if verbose:\n",
    "            log(\"molded_images\", molded_images)\n",
    "            log(\"image_metas\", image_metas)\n",
    "            log(\"anchors\", anchors)\n",
    "        # Run object detection\n",
    "        detections, _, _, mrcnn_mask, _, _, _ =\\\n",
    "            self.keras_model.predict([molded_images, image_metas, anchors], verbose=0)\n",
    "        # Process detections\n",
    "        results = []\n",
    "        for i, image in enumerate(images):\n",
    "            final_rois, final_class_ids, final_scores, final_masks =\\\n",
    "                self.unmold_detections(detections[i], mrcnn_mask[i],\n",
    "                                       image.shape, molded_images[i].shape,\n",
    "                                       windows[i])\n",
    "            results.append({\n",
    "                \"rois\": final_rois,\n",
    "                \"class_ids\": final_class_ids,\n",
    "                \"scores\": final_scores,\n",
    "                \"masks\": final_masks,\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def detect_molded(self, molded_images, image_metas, verbose=0):\n",
    "        \"\"\"Runs the detection pipeline, but expect inputs that are\n",
    "        molded already. Used mostly for debugging and inspecting\n",
    "        the model.\n",
    "\n",
    "        molded_images: List of images loaded using load_image_gt()\n",
    "        image_metas: image meta data, also returned by load_image_gt()\n",
    "\n",
    "        Returns a list of dicts, one dict per image. The dict contains:\n",
    "        rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n",
    "        class_ids: [N] int class IDs\n",
    "        scores: [N] float probability scores for the class IDs\n",
    "        masks: [H, W, N] instance binary masks\n",
    "        \"\"\"\n",
    "        assert self.mode == \"inference\", \"Create model in inference mode.\"\n",
    "        assert len(molded_images) == self.config.BATCH_SIZE,\\\n",
    "            \"Number of images must be equal to BATCH_SIZE\"\n",
    "\n",
    "        if verbose:\n",
    "            log(\"Processing {} images\".format(len(molded_images)))\n",
    "            for image in molded_images:\n",
    "                log(\"image\", image)\n",
    "\n",
    "        # Validate image sizes\n",
    "        # All images in a batch MUST be of the same size\n",
    "        image_shape = molded_images[0].shape\n",
    "        for g in molded_images[1:]:\n",
    "            assert g.shape == image_shape, \"Images must have the same size\"\n",
    "\n",
    "        # Anchors\n",
    "        anchors = self.get_anchors(image_shape)\n",
    "        # Duplicate across the batch dimension because Keras requires it\n",
    "        # TODO: can this be optimized to avoid duplicating the anchors?\n",
    "        anchors = np.broadcast_to(anchors, (self.config.BATCH_SIZE,) + anchors.shape)\n",
    "\n",
    "        if verbose:\n",
    "            log(\"molded_images\", molded_images)\n",
    "            log(\"image_metas\", image_metas)\n",
    "            log(\"anchors\", anchors)\n",
    "        # Run object detection\n",
    "        detections, _, _, mrcnn_mask, _, _, _ =\\\n",
    "            self.keras_model.predict([molded_images, image_metas, anchors], verbose=0)\n",
    "        # Process detections\n",
    "        results = []\n",
    "        for i, image in enumerate(molded_images):\n",
    "            window = [0, 0, image.shape[0], image.shape[1]]\n",
    "            final_rois, final_class_ids, final_scores, final_masks =\\\n",
    "                self.unmold_detections(detections[i], mrcnn_mask[i],\n",
    "                                       image.shape, molded_images[i].shape,\n",
    "                                       window)\n",
    "            results.append({\n",
    "                \"rois\": final_rois,\n",
    "                \"class_ids\": final_class_ids,\n",
    "                \"scores\": final_scores,\n",
    "                \"masks\": final_masks,\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def get_anchors(self, image_shape):\n",
    "        \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n",
    "        backbone_shapes = compute_backbone_shapes(self.config, image_shape)\n",
    "        # Cache anchors and reuse if image shape is the same\n",
    "        if not hasattr(self, \"_anchor_cache\"):\n",
    "            self._anchor_cache = {}\n",
    "        if not tuple(image_shape) in self._anchor_cache:\n",
    "            # Generate Anchors\n",
    "            a = utils.generate_pyramid_anchors(\n",
    "                self.config.RPN_ANCHOR_SCALES,\n",
    "                self.config.RPN_ANCHOR_RATIOS,\n",
    "                backbone_shapes,\n",
    "                self.config.BACKBONE_STRIDES,\n",
    "                self.config.RPN_ANCHOR_STRIDE)\n",
    "            # Keep a copy of the latest anchors in pixel coordinates because\n",
    "            # it's used in inspect_model notebooks.\n",
    "            # TODO: Remove this after the notebook are refactored to not use it\n",
    "            self.anchors = a\n",
    "            # Normalize coordinates\n",
    "            self._anchor_cache[tuple(image_shape)] = utils.norm_boxes(a, image_shape[:2])\n",
    "        return self._anchor_cache[tuple(image_shape)]\n",
    "\n",
    "    def ancestor(self, tensor, name, checked=None):\n",
    "        \"\"\"Finds the ancestor of a TF tensor in the computation graph.\n",
    "        tensor: TensorFlow symbolic tensor.\n",
    "        name: Name of ancestor tensor to find\n",
    "        checked: For internal use. A list of tensors that were already\n",
    "                 searched to avoid loops in traversing the graph.\n",
    "        \"\"\"\n",
    "        checked = checked if checked is not None else []\n",
    "        # Put a limit on how deep we go to avoid very long loops\n",
    "        if len(checked) > 500:\n",
    "            return None\n",
    "        # Convert name to a regex and allow matching a number prefix\n",
    "        # because Keras adds them automatically\n",
    "        if isinstance(name, str):\n",
    "            name = re.compile(name.replace(\"/\", r\"(\\_\\d+)*/\"))\n",
    "\n",
    "        parents = tensor.op.inputs\n",
    "        for p in parents:\n",
    "            if p in checked:\n",
    "                continue\n",
    "            if bool(re.fullmatch(name, p.name)):\n",
    "                return p\n",
    "            checked.append(p)\n",
    "            a = self.ancestor(p, name, checked)\n",
    "            if a is not None:\n",
    "                return a\n",
    "        return None\n",
    "\n",
    "    def find_trainable_layer(self, layer):\n",
    "        \"\"\"If a layer is encapsulated by another layer, this function\n",
    "        digs through the encapsulation and returns the layer that holds\n",
    "        the weights.\n",
    "        \"\"\"\n",
    "        if layer.__class__.__name__ == 'TimeDistributed':\n",
    "            return self.find_trainable_layer(layer.layer)\n",
    "        return layer\n",
    "\n",
    "    def get_trainable_layers(self):\n",
    "        \"\"\"Returns a list of layers that have weights.\"\"\"\n",
    "        layers = []\n",
    "        # Loop through all layers\n",
    "        for l in self.keras_model.layers:\n",
    "            # If layer is a wrapper, find inner trainable layer\n",
    "            l = self.find_trainable_layer(l)\n",
    "            # Include layer if it has weights\n",
    "            if l.get_weights():\n",
    "                layers.append(l)\n",
    "        return layers\n",
    "\n",
    "    def run_graph(self, images, outputs, image_metas=None):\n",
    "        \"\"\"Runs a sub-set of the computation graph that computes the given\n",
    "        outputs.\n",
    "\n",
    "        image_metas: If provided, the images are assumed to be already\n",
    "            molded (i.e. resized, padded, and normalized)\n",
    "\n",
    "        outputs: List of tuples (name, tensor) to compute. The tensors are\n",
    "            symbolic TensorFlow tensors and the names are for easy tracking.\n",
    "\n",
    "        Returns an ordered dict of results. Keys are the names received in the\n",
    "        input and values are Numpy arrays.\n",
    "        \"\"\"\n",
    "        model = self.keras_model\n",
    "\n",
    "        # Organize desired outputs into an ordered dict\n",
    "        outputs = OrderedDict(outputs)\n",
    "        for o in outputs.values():\n",
    "            assert o is not None\n",
    "\n",
    "        # Build a Keras function to run parts of the computation graph\n",
    "        inputs = model.inputs\n",
    "        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n",
    "            inputs += [K.learning_phase()]\n",
    "        kf = K.function(model.inputs, list(outputs.values()))\n",
    "\n",
    "        # Prepare inputs\n",
    "        if image_metas is None:\n",
    "            molded_images, image_metas, _ = self.mold_inputs(images)\n",
    "        else:\n",
    "            molded_images = images\n",
    "        image_shape = molded_images[0].shape\n",
    "        # Anchors\n",
    "        anchors = self.get_anchors(image_shape)\n",
    "        # Duplicate across the batch dimension because Keras requires it\n",
    "        # TODO: can this be optimized to avoid duplicating the anchors?\n",
    "        anchors = np.broadcast_to(anchors, (self.config.BATCH_SIZE,) + anchors.shape)\n",
    "        model_in = [molded_images, image_metas, anchors]\n",
    "\n",
    "        # Run inference\n",
    "        if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n",
    "            model_in.append(0.)\n",
    "        outputs_np = kf(model_in)\n",
    "\n",
    "        # Pack the generated Numpy arrays into a a dict and log the results.\n",
    "        outputs_np = OrderedDict([(k, v)\n",
    "                                  for k, v in zip(outputs.keys(), outputs_np)])\n",
    "        for k, v in outputs_np.items():\n",
    "            log(k, v)\n",
    "        return outputs_np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
